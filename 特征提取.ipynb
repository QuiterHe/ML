{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "## 分类变量特征提取\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "onehot_encoder = DictVectorizer()\n",
    "instances = [{'city': 'Beijing'},{'city': 'Shanghai'},{'city': 'Chongqing'}, {'city': 'Beijing'},]\n",
    "print(onehot_encoder.fit_transform(instances).toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'unc': 9, u'ate': 0, u'the': 8, u'sanwich': 7}\n"
     ]
    }
   ],
   "source": [
    "## 文字特征提取\n",
    "### 词库表示法\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game',\n",
    "    'I ate a sanwich'\n",
    "]\n",
    "\n",
    "# corpus（文集）由两个文档组成，构成的词库表vocabulary包括8个词\n",
    "# UNC, played, Duke, in, basketball, lost, the, game\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用文档特征向量间的欧式距离表示文档间的相似度\n",
    "$$ d = \\| x_0 - x_1 \\| $$\n",
    "向量的\n",
    "$$ \\| x \\| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档0与文档1的距离[[ 2.44948974]]\n",
      "文档0与文档2的距离[[ 2.64575131]]\n",
      "文档1与文档2的距离[[ 2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "## 用欧式距离表示文档的相似度\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game',\n",
    "    'I ate a sanwich'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(corpus).todense()\n",
    "for x,y in [[0, 1], [0, 2], [1, 2]]:\n",
    "    dist = euclidean_distances(counts[x], counts[y])\n",
    "    print('文档{}与文档{}的距离{}'.format(x, y, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现实中的新闻报告组成的文集，其中词汇表维度成千上万。但是是一个巨大的稀疏矩阵。\n",
    "高维数据会导致如下问题：\n",
    "> 1. 占用更大的内存\n",
    "> 2. 维度灾难(高维数据需要更多的训练数据，数据不足时会导致过拟合)\n",
    "\n",
    "所以，需要对高维数据进行降维\n",
    "常用的一些降维手段有（针对文本处理）：\n",
    "> 1. 将单词全部转换为小写\n",
    "> 2. 停用词（文集常用词）\n",
    "> 3. 词根还原和词形还原（将单词从不同的时态、派生形式还原）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 3 1 1]]\n",
      "{u'sandwich': 2, u'wizard': 4, u'dog': 1, u'transfigured': 3, u'ate': 0}\n"
     ]
    }
   ],
   "source": [
    "## 词频\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化\n",
    "不同文章的长度不一样，不具有可比性。需要实现归一化。\n",
    "\n",
    "### 1.使用L2范数实现归一化:\n",
    "\n",
    ">$$ tf(t ,d) = \\frac{f(t, d) + 1}{\\| x \\|} $$\n",
    "$f(t, d)$是第d个文档第t个单词的频率，$\\| x \\|$是频率向量的L2范数\n",
    "\n",
    "### 2.对数词频\n",
    ">$$ tf(t, d) = log(f(t, d) + 1) $$\n",
    "\n",
    "### 3.词频放大\n",
    ">$$ tf(t, d) = 0.5 + \\frac{f(t ,d) + 1}{maxf(w, d):w\\in d} $$\n",
    "\n",
    "### IDF\n",
    "> $$ idf(t, d) = log(\\frac{N}{\\lvert 1 + d \\in D : t \\in d\\rvert}) $$\n",
    "$N$是文集中的文档数量，$d \\in D : t \\in d$表示包含单词t的文档数量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Digit:', 0)\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.]\n",
      " [  0.   0.  13.  15.  10.  15.   5.   0.]\n",
      " [  0.   3.  15.   2.   0.  11.   8.   0.]\n",
      " [  0.   4.  12.   0.   0.   8.   8.   0.]\n",
      " [  0.   5.   8.   0.   0.   9.   8.   0.]\n",
      " [  0.   4.  11.   0.   1.  12.   7.   0.]\n",
      " [  0.   2.  14.   5.  10.  12.   0.   0.]\n",
      " [  0.   0.   6.  13.  10.   0.   0.   0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABARJREFUeJzt3VFNK1gUhtHD5BooEpBAsVAJYAUL1YCE1gqVUJBQCR0D\nk5B56Aa+u9Yz6d8EvpwXkn13vV4X0PTPd38B4HYEDmEChzCBQ5jAIUzgECZwCBM4hAkcwv7c6HOT\n/x53PB5H915fX8e2drvd2NZ+vx/b2mw2Y1vf4O6rH/CCQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFD\nmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIexWp4uSJk8J\nrbXW5+fn2Nblchnbur+/H9s6HA5jW2ut9fz8PLr3FS84hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziE\nCRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwn796aLT6TS2\nNXlKaK21zufz2NbDw8PY1m63G9ua/PtYy+kiYJDAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQ\nJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgEPbrb5NdLpexrcfH\nx7GttWbvhU3abrff/RX+Gl5wCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOY\nwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hDld9D/sdruxrbLJ39lmsxnb+om84BAm\ncAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5h\nAocwgUOYwCFM4BAmcAj79aeLJk/TnE6nsa1pk+eE3t/fx7ZeXl7Gtn4iLziECRzCBA5hAocwgUOY\nwCFM4BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziE\nCRzC7q7X6y0+9yYf+l8+Pj6mptZ2ux3bWmutt7e3sa3j8Ti2dT6fx7bK56bWWndf/YAXHMIEDmEC\nhzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAm\ncAgTOIQJHMIEDmECh7Bff5ts0uStsLXW2u/3Y1tPT09jW4fDYWwrzm0y+JsJHMIEDmEChzCBQ5jA\nIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQJnAIEziECRzCBA5hAocwgUOYwCFM4BAmcAgTOIQJ\nHMJudboI+AG84BAmcAgTOIQJHMIEDmEChzCBQ5jAIUzgECZwCBM4hAkcwgQOYQKHMIFDmMAhTOAQ\nJnAIEziECRzCBA5hAocwgUPYvwsCUvjCkAV5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f8bc9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 图片特征提取\n",
    "### 一张图片可以看成是一个每个元素都是颜色值的矩阵 OCR(Optical character recognition ， 光学文字识别)\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "digits = datasets.load_digits()\n",
    "print('Digit:', digits.target[0])\n",
    "print(digits.images[0])\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 兴趣点提取\n",
    "### 边（edges）\n",
    "> 像素快速变化的分界线\n",
    "\n",
    "### 角（corners）\n",
    "> 两条边的交集"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
